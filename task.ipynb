{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSAgQpHV-dAM"
      },
      "outputs": [],
      "source": [
        "!pip install beir transformers sentence-transformers --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import the dataset in beir standard format i .e it has three parts corpus , queries and qrels"
      ],
      "metadata": {
        "id": "nOPjSi5mMzLd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4uAdliy-ryc"
      },
      "outputs": [],
      "source": [
        "from beir import util, LoggingHandler\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "\n",
        "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/hotpotqa.zip\"\n",
        "out_dir = f\"./hotpotqa\"\n",
        "data_path = util.download_and_unzip(url, out_dir)\n",
        "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
        "print(f\"Sample Query: {list(queries.values())[0]}\")\n",
        "print(f\"Sample Corpus: {list(corpus.values())[0]}\")\n",
        "print(f\"Sample qrel: {list(qrels.values())[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we import the models and generate queru embedding and corpus embedding and find the cosine similarity between them, then we generate top 10 passages which are most similar"
      ],
      "metadata": {
        "id": "mo0yduimPHiK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNK2Fy3zrnlm"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "smallembmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "largeembmodel = SentenceTransformer('Snowflake/snowflake-arctic-embed-l')\n",
        "# Here i tried to do the corpus encoding in batches if GPU memory runs out\n",
        "# def encode_in_batches(corpus_texts, model, batch_size=64):\n",
        "#     corpus_embeddings = []\n",
        "#     for i in range(0, len(corpus_texts), batch_size):\n",
        "#         batch_texts = corpus_texts[i:i + batch_size]\n",
        "#         batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
        "#         corpus_embeddings.append(batch_embeddings)\n",
        "#     return torch.cat(corpus_embeddings, dim=0)\n",
        "\n",
        "def retrieve_candidates(query, model):\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    corpus_embeddings = model.encode([doc['text'] for doc_id, doc in corpus.items()], convert_to_tensor=True)\n",
        "    # corpus_texts = [doc['text'] for doc_id, doc in corpus.items()]\n",
        "    # corpus_embeddings = encode_in_batches(corpus_texts, model, batch_size)\n",
        "    scores = torch.matmul(query_embedding, corpus_embeddings.T).squeeze(0)\n",
        "    top_k_indices = torch.topk(scores, k=10).indices\n",
        "    top_k_passages = [list(corpus.values())[i]['text'] for i in top_k_indices]\n",
        "    return top_k_passages\n",
        "\n",
        "query = list(queries.values())[0]\n",
        "topkpassagessmall = retrieve_candidates(query, smallembmodel)\n",
        "topkpassageslarge = retrieve_candidates(query, largeembmodel)\n",
        "print(\"Top-k passages (small model):\", topkpassagessmall)\n",
        "print(\"Top-k passages (large model):\", topkpassageslarge)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here the ranker models are imported which will help in reranking the top-k passages for a given query, it first tokenize the queries, then it is passed throught the re ranked model after that it generates a relevance score according to that it the passages are rearranged"
      ],
      "metadata": {
        "id": "nd-f2UxZPvh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9DQmiVauUL4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "smallrerankmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
        "largererankmodel = AutoModelForSequenceClassification.from_pretrained('mixedbread-ai/mxbai-rerank-large-v1')\n",
        "smallreranktokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
        "largereranktokenizer = AutoTokenizer.from_pretrained('mixedbread-ai/mxbai-rerank-large-v1')\n",
        "\n",
        "def rerank_passages(query, passages, model, tokenizer):\n",
        "    inputs = tokenizer([query] * len(passages), passages, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    scores = outputs.logits[:, 0]\n",
        "    ranked_indices = torch.argsort(scores, descending=True)\n",
        "    ranked_passages = [passages[i] for i in ranked_indices]\n",
        "    return ranked_passages\n",
        "\n",
        "rankedpassagessmall = rerank_passages(query, topkpassagessmall, smallrerankmodel, smallreranktokenizer)\n",
        "rankedpassageslarge = rerank_passages(query, topkpassageslarge, largererankmodel, largereranktokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here the relevance labels are generated based on the ground truth data (qrels), and the ranking quality is measured against an ideal ranking using NDCG."
      ],
      "metadata": {
        "id": "O5T3Uq8MRg3k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7NkYtezuW0A"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "import numpy as np\n",
        "\n",
        "def create_relevance_labels(query_id, corpus_ids, qrels):\n",
        "    relevance_labels = []\n",
        "    for doc_id in corpus_ids:\n",
        "        if doc_id in qrels.get(query_id, {}):\n",
        "            relevance_labels.append(1)\n",
        "        else:\n",
        "            relevance_labels.append(0)\n",
        "    return relevance_labels\n",
        "\n",
        "def compute_ndcg_at_k(query_id, passages, top_k_passages, model_name, qrels):\n",
        "    top_k_doc_ids = [list(corpus.keys())[list(corpus.values()).index({'text': passage})] for passage in top_k_passages]\n",
        "    relevance_labels = create_relevance_labels(query_id, top_k_doc_ids, qrels)\n",
        "    ndcg = ndcg_score([relevance_labels], [list(range(len(top_k_doc_ids), 0, -1))])\n",
        "    print(f\"NDCG@10 ({model_name}): {ndcg}\")\n",
        "    return ndcg\n",
        "\n",
        "query_id = list(queries.keys())[0]\n",
        "compute_ndcg_at_k(query_id, topkpassagessmall, rankedpassagessmall, \"Small Model\", qrels)\n",
        "compute_ndcg_at_k(query_id, topkpassageslarge, rankedpassageslarge, \"Large Model\", qrels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I tried to run the model but after some time aprox 2 hrs into training i got an Out Of CUDA memory error , i also tried running in batches with batch size of 64 but the error still prevails . Therefore I was not able to generate the output .**"
      ],
      "metadata": {
        "id": "gOH6ntaDTIcg"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}